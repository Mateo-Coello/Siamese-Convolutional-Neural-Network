{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Libraries required"
      ],
      "metadata": {
        "id": "9aLD5KCOulXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XocmlBDnvGws"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Reshape, Concatenate, Activation, MaxPooling2D, Flatten, Dense, Dropout, Lambda\n",
        "from keras.applications.resnet import ResNet50\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully connected network used to classify the final output vector after the feature maps output by the pre-trained models are concatenated"
      ],
      "metadata": {
        "id": "LoJIeIZi9G45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc():\n",
        "  model = Sequential([\n",
        "      Flatten(),\n",
        "      Dense(1000, kernel_initializer=tf.keras.initializers.GlorotNormal(seed=0)),\n",
        "      Activation('relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(7, kernel_initializer=tf.keras.initializers.GlorotNormal(seed=0)),\n",
        "      Activation('softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "hftC3FY39Fc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connection to Google Drive to access the 224x224 variant of the dataset"
      ],
      "metadata": {
        "id": "CG2WJVELu_lB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcIx5nqcp8LV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the two inputs of the Siamese CNN, the two validation inputs, and the training and validation labels."
      ],
      "metadata": {
        "id": "7HmyPY0cwpBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRQ9CMEE2nkU"
      },
      "outputs": [],
      "source": [
        "train_a = np.load('/content/gdrive/MyDrive/224X224/x_Train_A.npy')\n",
        "train_b = np.load('/content/gdrive/MyDrive/224X224/x_Train_B.npy')\n",
        "labels = np.load('/content/gdrive/MyDrive/224X224/y_Train.npy')\n",
        "val_a = np.load('/content/gdrive/MyDrive/224X224/x_Test_A.npy')\n",
        "val_b = np.load('/content/gdrive/MyDrive/224X224/x_Test_B.npy')\n",
        "val_labels = np.load('/content/gdrive/MyDrive/224X224/y_Test.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting training and validation labels into categories."
      ],
      "metadata": {
        "id": "kzx-9G_mw1gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWKoiR3zAW_V"
      },
      "outputs": [],
      "source": [
        "labels = to_categorical(labels)\n",
        "val_labels = to_categorical(val_labels)\n",
        "\n",
        "print(labels.shape)\n",
        "print(val_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the Siamese CNN model that implements the ResNet50 pre-trained model\n"
      ],
      "metadata": {
        "id": "cItTkVjuxGF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8KUlQ1RnlFz"
      },
      "outputs": [],
      "source": [
        "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "resnet.trainable = True\n",
        "\n",
        "left_input = Input(shape=(224,224,3))\n",
        "right_input = Input(shape=(224,224,3))\n",
        "\n",
        "left_features = resnet(left_input)\n",
        "right_features = resnet(right_input)\n",
        "print(left_features.shape)\n",
        "\n",
        "merged_features = tf.concat([left_features, right_features], axis=-1)\n",
        "print(merged_features.shape)\n",
        "\n",
        "fc_net = fc()\n",
        "output = fc_net(merged_features)\n",
        "print(output.shape)\n",
        "\n",
        "model = Model(inputs=[left_input, right_input], outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of the model and definition of checkpoint method to save the best results obtained by the model."
      ],
      "metadata": {
        "id": "mud7fjITxKEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUAXJFxfYs-e"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "filepath = \"/content/gdrive/My Drive/best_weights_resnet50.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, \n",
        "                             save_best_only=True, \n",
        "                             save_weights_only=False, \n",
        "                             monitor='val_loss', \n",
        "                             mode='min', \n",
        "                             verbose=1)\n",
        "\n",
        "history = model.fit([train_a, train_b],\n",
        "           labels,\n",
        "           epochs=50,\n",
        "           batch_size=32,\n",
        "           shuffle=True,\n",
        "           validation_data=([val_a, val_b], val_labels),\n",
        "           callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the results of accuracy and loss after the model has finished training"
      ],
      "metadata": {
        "id": "2RWDKzIJxr5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLUy7vqcRkQD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observng the highest accuracy and lowest loss values of the model"
      ],
      "metadata": {
        "id": "Yq0lxUSMx_l2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlaToUZYbkSu"
      },
      "outputs": [],
      "source": [
        "best_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "# Find the index of the highest accuracy value\n",
        "index = history.history['val_accuracy'].index(best_accuracy)\n",
        "\n",
        "# Get the corresponding loss value\n",
        "loss_f_best_acc = history.history['val_loss'][index]\n",
        "\n",
        "print('Best Accuracy:', best_accuracy, 'Loss:', loss_f_best_acc)\n",
        "\n",
        "best_loss = min(history.history['val_loss'])\n",
        "\n",
        "# Find the index of the minimum loss value\n",
        "index = history.history['val_loss'].index(best_loss)\n",
        "\n",
        "# Get the corresponding loss value\n",
        "acc_f_min_loss = history.history['val_accuracy'][index]\n",
        "\n",
        "print('Best Loss:', best_loss, 'Accuracy:', acc_f_min_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}