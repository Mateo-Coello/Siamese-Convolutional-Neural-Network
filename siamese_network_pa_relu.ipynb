{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aLD5KCOulXq"
      },
      "source": [
        "Libraries required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XocmlBDnvGws"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Concatenate, Activation, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BVH6AzjJusDe"
      },
      "source": [
        "Functions that create the layers of the proposed architecture - ReLU variant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5sSztTnNH7C"
      },
      "outputs": [],
      "source": [
        "def part1():\n",
        "  model = Sequential([\n",
        "      # Layer 1\n",
        "      Conv2D(15, (3, 3), padding='same', input_shape=(224, 224, 3), kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "\n",
        "      # Layer 2\n",
        "      Conv2D(30, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "      MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "      # Layer 3\n",
        "      Conv2D(45, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "\n",
        "      # Layer 4\n",
        "      Conv2D(60, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "      MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "\n",
        "      # Layer 5\n",
        "      Conv2D(75, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "\n",
        "      # Layer 6\n",
        "      Conv2D(90, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "      MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "def part2():\n",
        "  model = Sequential([\n",
        "      # Layer 7\n",
        "      Conv2D(105, (5, 5), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "      MaxPooling2D(pool_size=(2,2), padding='same'),\n",
        "      Dropout(0.1),\n",
        "      \n",
        "      # Layer 8\n",
        "      Conv2D(90, (5, 5), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "      MaxPooling2D(pool_size=(2,2), padding='same'),\n",
        "      Dropout(0.1),\n",
        "\n",
        "      # Layer 9\n",
        "      Conv2D(75, (5, 5), padding='same', kernel_initializer='he_normal'),\n",
        "      BatchNormalization(),\n",
        "      Activation('relu'),\n",
        "      MaxPooling2D(pool_size=(2,2), padding='same'),\n",
        "      Dropout(0.1),\n",
        "\n",
        "      #FC\n",
        "      Flatten(),\n",
        "      Dense(1000, kernel_initializer='glorot_normal'),\n",
        "      Activation('relu'),\n",
        "      Dense(7, kernel_initializer='glorot_normal'),\n",
        "      Activation('softmax'),\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG2WJVELu_lB"
      },
      "source": [
        "Connection to Google Drive to access the 224x224 variant of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcIx5nqcp8LV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HmyPY0cwpBF"
      },
      "source": [
        "Loading the two inputs of the Siamese CNN, the two validation inputs, and the training and validation labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRQ9CMEE2nkU"
      },
      "outputs": [],
      "source": [
        "train_a = np.load('/content/gdrive/MyDrive/224X224/x_Train_A.npy')\n",
        "train_b = np.load('/content/gdrive/MyDrive/224X224/x_Train_B.npy')\n",
        "labels = np.load('/content/gdrive/MyDrive/224X224/y_Train.npy')\n",
        "val_a = np.load('/content/gdrive/MyDrive/224X224/x_Test_A.npy')\n",
        "val_b = np.load('/content/gdrive/MyDrive/224X224/x_Test_B.npy')\n",
        "val_labels = np.load('/content/gdrive/MyDrive/224X224/y_Test.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzx-9G_mw1gg"
      },
      "source": [
        "Converting training and validation labels into categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWKoiR3zAW_V",
        "outputId": "0d002eaf-2b75-4edc-b613-d08242173f49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1139, 7)\n",
            "(126, 7)\n"
          ]
        }
      ],
      "source": [
        "labels = to_categorical(labels)\n",
        "val_labels = to_categorical(val_labels)\n",
        "\n",
        "print(labels.shape)\n",
        "print(val_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cItTkVjuxGF1"
      },
      "source": [
        "Definition of the Siamese CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8KUlQ1RnlFz",
        "outputId": "f9a3e611-7fc7-4d92-e98c-42e860fa903f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 9, 9, 60)\n",
            "(None, 9, 9, 120)\n",
            "(None, 7)\n"
          ]
        }
      ],
      "source": [
        "left_input = Input(shape=(224,224,3))\n",
        "right_input = Input(shape=(224,224,3))\n",
        "\n",
        "siamese_model = part1()\n",
        "left_features = siamese_model(left_input)\n",
        "right_features = siamese_model(right_input)\n",
        "print(left_features.shape)\n",
        "\n",
        "merged_features = tf.concat([left_features, right_features], axis=-1)\n",
        "print(merged_features.shape)\n",
        "\n",
        "conv_model = part2()\n",
        "conv_output = conv_model(merged_features)\n",
        "print(conv_output.shape)\n",
        "\n",
        "model = Model(inputs=[left_input, right_input], outputs=conv_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mud7fjITxKEg"
      },
      "source": [
        "Training of the model and definition of checkpoint method to save the best results obtained by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUAXJFxfYs-e"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "filepath = \"/content/gdrive/My Drive/best_weights_pa_relu.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, \n",
        "                             save_best_only=True, \n",
        "                             save_weights_only=False, \n",
        "                             monitor='val_loss', \n",
        "                             mode='min', \n",
        "                             verbose=1)\n",
        "\n",
        "history = model.fit([train_a, train_b],\n",
        "           labels,\n",
        "           epochs=100,\n",
        "           batch_size=32,\n",
        "           shuffle=True,\n",
        "           validation_data=([val_a, val_b], val_labels),\n",
        "           callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RWDKzIJxr5b"
      },
      "source": [
        "Plotting the results of accuracy and loss after the model has finished training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLUy7vqcRkQD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq0lxUSMx_l2"
      },
      "source": [
        "Observng the highest accuracy and lowest loss values of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlaToUZYbkSu"
      },
      "outputs": [],
      "source": [
        "best_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "# Find the index of the highest accuracy value\n",
        "index = history.history['val_accuracy'].index(best_accuracy)\n",
        "\n",
        "# Get the corresponding loss value\n",
        "loss_f_best_acc = history.history['val_loss'][index]\n",
        "\n",
        "print('Best Accuracy:', best_accuracy, 'Loss:', loss_f_best_acc)\n",
        "\n",
        "best_loss = min(history.history['val_loss'])\n",
        "\n",
        "# Find the index of the minimum loss value\n",
        "index = history.history['val_loss'].index(best_loss)\n",
        "\n",
        "# Get the corresponding loss value\n",
        "acc_f_min_loss = history.history['val_accuracy'][index]\n",
        "\n",
        "print('Best Loss:', best_loss, 'Accuracy:', acc_f_min_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
